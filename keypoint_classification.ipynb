{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T07:50:07.883815Z",
     "iopub.status.busy": "2025-04-07T07:50:07.883466Z",
     "iopub.status.idle": "2025-04-07T07:50:07.888316Z",
     "shell.execute_reply": "2025-04-07T07:50:07.887356Z",
     "shell.execute_reply.started": "2025-04-07T07:50:07.883785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch.optim as optim  \n",
    "from sklearn.model_selection import train_test_split \n",
    "random_seed = 42 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.Specify each path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T07:40:00.799191Z",
     "iopub.status.busy": "2025-04-07T07:40:00.798877Z",
     "iopub.status.idle": "2025-04-07T07:40:00.803008Z",
     "shell.execute_reply": "2025-04-07T07:40:00.802249Z",
     "shell.execute_reply.started": "2025-04-07T07:40:00.799166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\T'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14872\\2977927794.py:1: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  dataset = 'F:\\TTCS\\model\\keypoint_classifier\\keypoint.csv'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14872\\2977927794.py:2: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  model_save_path = 'F:\\TTCS\\model\\keypoint_classifier\\weights_model.pth'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14872\\2977927794.py:3: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  pth_save_path = 'F:\\TTCS\\model\\keypoint_classifier\\converted_model.pth'\n"
     ]
    }
   ],
   "source": [
    "dataset = 'F:\\TTCS\\model\\keypoint_classifier\\keypoint.csv'\n",
    "model_save_path = 'F:\\TTCS\\model\\keypoint_classifier\\weights_model.pth'\n",
    "pth_save_path = 'F:\\TTCS\\model\\keypoint_classifier\\converted_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T07:40:02.475699Z",
     "iopub.status.busy": "2025-04-07T07:40:02.475417Z",
     "iopub.status.idle": "2025-04-07T07:40:02.479101Z",
     "shell.execute_reply": "2025-04-07T07:40:02.478252Z",
     "shell.execute_reply.started": "2025-04-07T07:40:02.475675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T07:40:03.891314Z",
     "iopub.status.busy": "2025-04-07T07:40:03.890988Z",
     "iopub.status.idle": "2025-04-07T07:40:03.990467Z",
     "shell.execute_reply": "2025-04-07T07:40:03.989547Z",
     "shell.execute_reply.started": "2025-04-07T07:40:03.891283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_dataset = np.loadtxt(dataset, delimiter = ',', dtype = 'float32', usecols = list(range(1,(21*2)+1)))\n",
    "y_dataset = np.loadtxt(dataset, delimiter = ',', dtype = 'int32', usecols = (0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T07:51:44.526267Z",
     "iopub.status.busy": "2025-04-07T07:51:44.525923Z",
     "iopub.status.idle": "2025-04-07T07:51:44.532199Z",
     "shell.execute_reply": "2025-04-07T07:51:44.531305Z",
     "shell.execute_reply.started": "2025-04-07T07:51:44.526227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset,y_dataset,test_size = 0.25, random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T07:52:04.401512Z",
     "iopub.status.busy": "2025-04-07T07:52:04.401178Z",
     "iopub.status.idle": "2025-04-07T07:52:04.405921Z",
     "shell.execute_reply": "2025-04-07T07:52:04.404974Z",
     "shell.execute_reply.started": "2025-04-07T07:52:04.401474Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3656\n",
      "1219\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T07:46:02.968472Z",
     "iopub.status.busy": "2025-04-07T07:46:02.968118Z",
     "iopub.status.idle": "2025-04-07T07:46:02.997445Z",
     "shell.execute_reply": "2025-04-07T07:46:02.996610Z",
     "shell.execute_reply.started": "2025-04-07T07:46:02.968446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=42, out_features=20, bias=True)\n",
      "  (1): Dropout(p=0.2, inplace=False)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (4): Dropout(p=0.4, inplace=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (7): Softmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "myModel = nn.Sequential(\n",
    "    nn.Linear(42,20),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10),\n",
    "    nn.Dropout(p=0.4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,n_classes),\n",
    "    nn.Softmax(dim = -1)\n",
    ")\n",
    "print(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T07:53:26.480607Z",
     "iopub.status.busy": "2025-04-07T07:53:26.480282Z",
     "iopub.status.idle": "2025-04-07T07:53:26.490026Z",
     "shell.execute_reply": "2025-04-07T07:53:26.489234Z",
     "shell.execute_reply.started": "2025-04-07T07:53:26.480583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train),torch.from_numpy(y_train)),batch_size = 32,shuffle = True)\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(X_test),torch.from_numpy(y_test)),batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:06:26.978364Z",
     "iopub.status.busy": "2025-04-07T08:06:26.978001Z",
     "iopub.status.idle": "2025-04-07T08:06:26.983058Z",
     "shell.execute_reply": "2025-04-07T08:06:26.982093Z",
     "shell.execute_reply.started": "2025-04-07T08:06:26.978333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(myModel.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:50:40.398951Z",
     "iopub.status.busy": "2025-04-07T08:50:40.398660Z",
     "iopub.status.idle": "2025-04-07T08:50:48.024434Z",
     "shell.execute_reply": "2025-04-07T08:50:48.023701Z",
     "shell.execute_reply.started": "2025-04-07T08:50:40.398929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Val Loss: 1.0027 Accuracy: 89.6637\n",
      "ok\n",
      "Epoch: 2, Val Loss: 1.0023 Accuracy: 89.8277\n",
      "ok\n",
      "Epoch: 3, Val Loss: 1.0017 Accuracy: 89.8277\n",
      "ok\n",
      "Epoch: 4, Val Loss: 1.0029 Accuracy: 89.7457\n",
      "Epoch: 5, Val Loss: 1.0027 Accuracy: 89.7457\n",
      "Epoch: 6, Val Loss: 1.0015 Accuracy: 89.9098\n",
      "ok\n",
      "Epoch: 7, Val Loss: 1.0021 Accuracy: 89.7457\n",
      "Epoch: 8, Val Loss: 1.0020 Accuracy: 89.9098\n",
      "Epoch: 9, Val Loss: 1.0025 Accuracy: 89.7457\n",
      "Epoch: 10, Val Loss: 1.0026 Accuracy: 89.7457\n",
      "Epoch: 11, Val Loss: 1.0023 Accuracy: 89.8277\n",
      "Epoch: 12, Val Loss: 1.0025 Accuracy: 89.8277\n",
      "Epoch: 13, Val Loss: 1.0020 Accuracy: 89.8277\n",
      "Epoch: 14, Val Loss: 1.0017 Accuracy: 89.9098\n",
      "Epoch: 15, Val Loss: 1.0025 Accuracy: 89.8277\n",
      "Epoch: 16, Val Loss: 1.0016 Accuracy: 89.9098\n",
      "Epoch: 17, Val Loss: 1.0013 Accuracy: 89.9098\n",
      "ok\n",
      "Epoch: 18, Val Loss: 1.0017 Accuracy: 89.8277\n",
      "Epoch: 19, Val Loss: 1.0018 Accuracy: 89.9098\n",
      "Epoch: 20, Val Loss: 1.0021 Accuracy: 89.7457\n",
      "Epoch: 21, Val Loss: 1.0020 Accuracy: 89.7457\n",
      "Epoch: 22, Val Loss: 1.0011 Accuracy: 89.7457\n",
      "ok\n",
      "Epoch: 23, Val Loss: 1.0021 Accuracy: 89.7457\n",
      "Epoch: 24, Val Loss: 1.0021 Accuracy: 89.8277\n",
      "Epoch: 25, Val Loss: 1.0018 Accuracy: 89.8277\n",
      "Epoch: 26, Val Loss: 1.0021 Accuracy: 89.7457\n",
      "Epoch: 27, Val Loss: 1.0015 Accuracy: 89.8277\n",
      "Epoch: 28, Val Loss: 1.0006 Accuracy: 89.9098\n",
      "ok\n",
      "Epoch: 29, Val Loss: 1.0020 Accuracy: 89.7457\n",
      "Epoch: 30, Val Loss: 1.0011 Accuracy: 89.8277\n",
      "Epoch: 31, Val Loss: 1.0009 Accuracy: 89.8277\n",
      "Epoch: 32, Val Loss: 1.0009 Accuracy: 89.8277\n",
      "Epoch: 33, Val Loss: 1.0021 Accuracy: 89.8277\n",
      "Epoch: 34, Val Loss: 1.0021 Accuracy: 89.7457\n",
      "Epoch: 35, Val Loss: 1.0016 Accuracy: 89.7457\n",
      "Epoch: 36, Val Loss: 1.0016 Accuracy: 89.8277\n",
      "Epoch: 37, Val Loss: 1.0019 Accuracy: 89.7457\n",
      "Epoch: 38, Val Loss: 1.0023 Accuracy: 89.7457\n",
      "Epoch: 39, Val Loss: 1.0017 Accuracy: 89.7457\n",
      "Epoch: 40, Val Loss: 1.0015 Accuracy: 89.8277\n",
      "Epoch: 41, Val Loss: 1.0017 Accuracy: 89.8277\n",
      "Epoch: 42, Val Loss: 1.0018 Accuracy: 89.7457\n",
      "Epoch: 43, Val Loss: 1.0021 Accuracy: 89.8277\n",
      "Epoch: 44, Val Loss: 1.0020 Accuracy: 89.7457\n",
      "Epoch: 45, Val Loss: 1.0019 Accuracy: 89.7457\n",
      "Epoch: 46, Val Loss: 1.0014 Accuracy: 89.8277\n",
      "Epoch: 47, Val Loss: 1.0017 Accuracy: 89.7457\n",
      "Epoch: 48, Val Loss: 1.0018 Accuracy: 89.7457\n",
      "Epoch: 49, Val Loss: 1.0010 Accuracy: 89.8277\n",
      "Epoch: 50, Val Loss: 1.0021 Accuracy: 89.7457\n",
      "Epoch: 51, Val Loss: 1.0008 Accuracy: 89.8277\n",
      "Epoch: 52, Val Loss: 1.0020 Accuracy: 89.7457\n",
      "Epoch: 53, Val Loss: 1.0027 Accuracy: 89.7457\n",
      "Epoch: 54, Val Loss: 1.0019 Accuracy: 89.7457\n",
      "Epoch: 55, Val Loss: 1.0023 Accuracy: 89.7457\n",
      "Epoch: 56, Val Loss: 1.0014 Accuracy: 89.9098\n",
      "Epoch: 57, Val Loss: 1.0037 Accuracy: 89.7457\n",
      "Epoch: 58, Val Loss: 1.0021 Accuracy: 89.7457\n",
      "Epoch: 59, Val Loss: 1.0018 Accuracy: 89.7457\n",
      "Epoch: 60, Val Loss: 1.0024 Accuracy: 89.6637\n",
      "Epoch: 61, Val Loss: 1.0014 Accuracy: 89.7457\n",
      "Epoch: 62, Val Loss: 1.0012 Accuracy: 89.8277\n",
      "Epoch: 63, Val Loss: 1.0003 Accuracy: 89.9098\n",
      "ok\n",
      "Epoch: 64, Val Loss: 1.0014 Accuracy: 89.8277\n",
      "Epoch: 65, Val Loss: 1.0017 Accuracy: 89.8277\n",
      "Epoch: 66, Val Loss: 1.0036 Accuracy: 89.7457\n",
      "Epoch: 67, Val Loss: 1.0023 Accuracy: 89.7457\n",
      "Epoch: 68, Val Loss: 1.0004 Accuracy: 89.9098\n",
      "Epoch: 69, Val Loss: 1.0009 Accuracy: 89.9098\n",
      "Epoch: 70, Val Loss: 1.0007 Accuracy: 89.9098\n",
      "Epoch: 71, Val Loss: 1.0022 Accuracy: 89.9918\n",
      "Epoch: 72, Val Loss: 1.0009 Accuracy: 89.9098\n",
      "Epoch: 73, Val Loss: 1.0004 Accuracy: 89.9918\n",
      "Epoch: 74, Val Loss: 1.0004 Accuracy: 89.9098\n",
      "Epoch: 75, Val Loss: 1.0021 Accuracy: 89.8277\n",
      "Epoch: 76, Val Loss: 1.0007 Accuracy: 89.9098\n",
      "Epoch: 77, Val Loss: 1.0019 Accuracy: 89.6637\n",
      "Epoch: 78, Val Loss: 1.0017 Accuracy: 89.6637\n",
      "Epoch: 79, Val Loss: 1.0024 Accuracy: 89.7457\n",
      "Epoch: 80, Val Loss: 1.0024 Accuracy: 89.7457\n",
      "Epoch: 81, Val Loss: 1.0013 Accuracy: 89.9098\n",
      "Epoch: 82, Val Loss: 1.0023 Accuracy: 89.8277\n",
      "Epoch: 83, Val Loss: 1.0011 Accuracy: 89.9098\n",
      "Epoch: 84, Val Loss: 0.9996 Accuracy: 89.9918\n",
      "ok\n",
      "Epoch: 85, Val Loss: 1.0027 Accuracy: 89.9098\n",
      "Epoch: 86, Val Loss: 1.0022 Accuracy: 89.7457\n",
      "Epoch: 87, Val Loss: 1.0030 Accuracy: 89.7457\n",
      "Epoch: 88, Val Loss: 1.0022 Accuracy: 89.8277\n",
      "Epoch: 89, Val Loss: 1.0014 Accuracy: 89.9098\n",
      "Epoch: 90, Val Loss: 1.0006 Accuracy: 89.9098\n",
      "Epoch: 91, Val Loss: 0.9995 Accuracy: 89.9918\n",
      "ok\n",
      "Epoch: 92, Val Loss: 1.0005 Accuracy: 89.9098\n",
      "Epoch: 93, Val Loss: 1.0012 Accuracy: 89.8277\n",
      "Epoch: 94, Val Loss: 1.0016 Accuracy: 89.9098\n",
      "Epoch: 95, Val Loss: 1.0022 Accuracy: 89.8277\n",
      "Epoch: 96, Val Loss: 1.0000 Accuracy: 89.9918\n",
      "Epoch: 97, Val Loss: 1.0008 Accuracy: 89.8277\n",
      "Epoch: 98, Val Loss: 1.0012 Accuracy: 89.9098\n",
      "Epoch: 99, Val Loss: 1.0009 Accuracy: 89.9098\n",
      "Epoch: 100, Val Loss: 1.0017 Accuracy: 89.8277\n",
      "Epoch: 101, Val Loss: 1.0025 Accuracy: 89.7457\n",
      "Epoch: 102, Val Loss: 1.0005 Accuracy: 89.9098\n",
      "Epoch: 103, Val Loss: 1.0023 Accuracy: 89.7457\n",
      "Epoch: 104, Val Loss: 1.0028 Accuracy: 89.8277\n",
      "Epoch: 105, Val Loss: 1.0020 Accuracy: 89.7457\n",
      "Epoch: 106, Val Loss: 1.0039 Accuracy: 89.6637\n",
      "Epoch: 107, Val Loss: 1.0022 Accuracy: 89.7457\n",
      "Epoch: 108, Val Loss: 1.0035 Accuracy: 89.7457\n",
      "Epoch: 109, Val Loss: 1.0025 Accuracy: 89.8277\n",
      "Epoch: 110, Val Loss: 1.0022 Accuracy: 89.9098\n",
      "Epoch: 111, Val Loss: 1.0010 Accuracy: 89.9098\n",
      "Epoch: 112, Val Loss: 1.0021 Accuracy: 89.8277\n",
      "Epoch: 113, Val Loss: 1.0019 Accuracy: 89.7457\n",
      "Epoch: 114, Val Loss: 1.0022 Accuracy: 89.9098\n",
      "Epoch: 115, Val Loss: 1.0023 Accuracy: 89.9098\n",
      "Epoch: 116, Val Loss: 1.0024 Accuracy: 89.8277\n",
      "Epoch: 117, Val Loss: 1.0016 Accuracy: 89.9098\n",
      "Epoch: 118, Val Loss: 1.0022 Accuracy: 89.8277\n",
      "Epoch: 119, Val Loss: 1.0028 Accuracy: 89.8277\n",
      "Epoch: 120, Val Loss: 1.0006 Accuracy: 89.9918\n",
      "Epoch: 121, Val Loss: 0.9996 Accuracy: 89.9918\n",
      "Epoch: 122, Val Loss: 1.0009 Accuracy: 89.9098\n",
      "Epoch: 123, Val Loss: 1.0019 Accuracy: 89.9098\n",
      "Epoch: 124, Val Loss: 1.0017 Accuracy: 89.7457\n",
      "Epoch: 125, Val Loss: 1.0013 Accuracy: 89.7457\n",
      "Epoch: 126, Val Loss: 1.0017 Accuracy: 89.7457\n",
      "Epoch: 127, Val Loss: 1.0022 Accuracy: 89.8277\n",
      "Epoch: 128, Val Loss: 1.0013 Accuracy: 89.8277\n",
      "Epoch: 129, Val Loss: 1.0003 Accuracy: 89.9098\n",
      "Epoch: 130, Val Loss: 1.0011 Accuracy: 89.7457\n",
      "Epoch: 131, Val Loss: 1.0014 Accuracy: 89.9098\n",
      "Epoch: 132, Val Loss: 0.9996 Accuracy: 89.9918\n",
      "Epoch: 133, Val Loss: 1.0005 Accuracy: 89.9098\n",
      "Epoch: 134, Val Loss: 1.0009 Accuracy: 89.9098\n",
      "Epoch: 135, Val Loss: 1.0020 Accuracy: 89.8277\n",
      "Epoch: 136, Val Loss: 1.0001 Accuracy: 89.9918\n",
      "Epoch: 137, Val Loss: 1.0010 Accuracy: 89.9098\n",
      "Epoch: 138, Val Loss: 1.0006 Accuracy: 89.9098\n",
      "Epoch: 139, Val Loss: 0.9997 Accuracy: 89.9918\n",
      "Epoch: 140, Val Loss: 0.9997 Accuracy: 90.0738\n",
      "Epoch: 141, Val Loss: 0.9997 Accuracy: 90.0738\n",
      "Epoch: 142, Val Loss: 1.0006 Accuracy: 89.9918\n",
      "Epoch: 143, Val Loss: 1.0012 Accuracy: 89.9098\n",
      "Epoch: 144, Val Loss: 1.0011 Accuracy: 89.9918\n",
      "Epoch: 145, Val Loss: 1.0023 Accuracy: 89.8277\n",
      "Epoch: 146, Val Loss: 1.0019 Accuracy: 89.9098\n",
      "Epoch: 147, Val Loss: 1.0009 Accuracy: 89.9918\n",
      "Epoch: 148, Val Loss: 1.0022 Accuracy: 90.0738\n",
      "Epoch: 149, Val Loss: 1.0021 Accuracy: 89.8277\n",
      "Epoch: 150, Val Loss: 1.0008 Accuracy: 89.7457\n",
      "Epoch: 151, Val Loss: 1.0030 Accuracy: 89.7457\n",
      "Epoch: 152, Val Loss: 1.0022 Accuracy: 89.8277\n",
      "Epoch: 153, Val Loss: 1.0024 Accuracy: 89.8277\n",
      "Epoch: 154, Val Loss: 0.9997 Accuracy: 89.9098\n",
      "Epoch: 155, Val Loss: 1.0016 Accuracy: 89.8277\n",
      "Epoch: 156, Val Loss: 1.0007 Accuracy: 89.8277\n",
      "Epoch: 157, Val Loss: 1.0015 Accuracy: 89.9098\n",
      "Epoch: 158, Val Loss: 1.0014 Accuracy: 89.9918\n",
      "Epoch: 159, Val Loss: 0.9998 Accuracy: 89.9918\n",
      "Epoch: 160, Val Loss: 0.9989 Accuracy: 90.0738\n",
      "ok\n",
      "Epoch: 161, Val Loss: 0.9993 Accuracy: 90.0738\n",
      "Epoch: 162, Val Loss: 0.9995 Accuracy: 89.9098\n",
      "Epoch: 163, Val Loss: 1.0005 Accuracy: 89.9098\n",
      "Epoch: 164, Val Loss: 1.0007 Accuracy: 89.9918\n",
      "Epoch: 165, Val Loss: 1.0000 Accuracy: 89.9918\n",
      "Epoch: 166, Val Loss: 1.0005 Accuracy: 89.9098\n",
      "Epoch: 167, Val Loss: 1.0004 Accuracy: 89.8277\n",
      "Epoch: 168, Val Loss: 1.0006 Accuracy: 89.9098\n",
      "Epoch: 169, Val Loss: 1.0002 Accuracy: 89.9098\n",
      "Epoch: 170, Val Loss: 1.0004 Accuracy: 89.8277\n",
      "Epoch: 171, Val Loss: 1.0006 Accuracy: 89.7457\n",
      "Epoch: 172, Val Loss: 1.0005 Accuracy: 89.8277\n",
      "Epoch: 173, Val Loss: 0.9998 Accuracy: 89.9098\n",
      "Epoch: 174, Val Loss: 1.0009 Accuracy: 89.8277\n",
      "Epoch: 175, Val Loss: 1.0012 Accuracy: 89.7457\n",
      "Epoch: 176, Val Loss: 1.0012 Accuracy: 89.7457\n",
      "Epoch: 177, Val Loss: 1.0006 Accuracy: 89.9098\n",
      "Epoch: 178, Val Loss: 1.0000 Accuracy: 89.9098\n",
      "Epoch: 179, Val Loss: 1.0003 Accuracy: 89.9918\n",
      "Epoch: 180, Val Loss: 0.9993 Accuracy: 89.9098\n",
      "Epoch: 181, Val Loss: 1.0014 Accuracy: 89.8277\n",
      "Epoch: 182, Val Loss: 1.0013 Accuracy: 89.8277\n",
      "Epoch: 183, Val Loss: 1.0015 Accuracy: 89.8277\n",
      "Epoch: 184, Val Loss: 1.0000 Accuracy: 89.9098\n",
      "Epoch: 185, Val Loss: 1.0002 Accuracy: 89.8277\n",
      "Epoch: 186, Val Loss: 1.0019 Accuracy: 89.8277\n",
      "Epoch: 187, Val Loss: 1.0002 Accuracy: 89.9098\n",
      "Epoch: 188, Val Loss: 1.0011 Accuracy: 89.9098\n",
      "Epoch: 189, Val Loss: 1.0031 Accuracy: 89.7457\n",
      "Epoch: 190, Val Loss: 1.0000 Accuracy: 89.9098\n",
      "Epoch: 191, Val Loss: 1.0022 Accuracy: 89.7457\n",
      "Epoch: 192, Val Loss: 1.0004 Accuracy: 89.8277\n",
      "Epoch: 193, Val Loss: 1.0008 Accuracy: 89.8277\n",
      "Epoch: 194, Val Loss: 1.0015 Accuracy: 89.7457\n",
      "Epoch: 195, Val Loss: 1.0032 Accuracy: 89.5816\n",
      "Epoch: 196, Val Loss: 1.0026 Accuracy: 89.7457\n",
      "Epoch: 197, Val Loss: 1.0017 Accuracy: 89.7457\n",
      "Epoch: 198, Val Loss: 1.0019 Accuracy: 89.8277\n",
      "Epoch: 199, Val Loss: 1.0005 Accuracy: 89.9098\n",
      "Epoch: 200, Val Loss: 1.0010 Accuracy: 89.8277\n",
      "Epoch: 201, Val Loss: 1.0026 Accuracy: 89.7457\n",
      "Epoch: 202, Val Loss: 1.0008 Accuracy: 89.8277\n",
      "Epoch: 203, Val Loss: 1.0017 Accuracy: 89.7457\n",
      "Epoch: 204, Val Loss: 1.0019 Accuracy: 89.6637\n",
      "Epoch: 205, Val Loss: 1.0007 Accuracy: 89.9098\n",
      "Epoch: 206, Val Loss: 1.0012 Accuracy: 89.8277\n",
      "Epoch: 207, Val Loss: 1.0004 Accuracy: 89.9098\n",
      "Epoch: 208, Val Loss: 1.0027 Accuracy: 89.7457\n",
      "Epoch: 209, Val Loss: 1.0007 Accuracy: 89.9098\n",
      "Epoch: 210, Val Loss: 1.0015 Accuracy: 89.8277\n",
      "Epoch: 211, Val Loss: 1.0014 Accuracy: 89.7457\n",
      "Epoch: 212, Val Loss: 1.0029 Accuracy: 89.6637\n",
      "Epoch: 213, Val Loss: 1.0019 Accuracy: 89.7457\n",
      "Epoch: 214, Val Loss: 1.0014 Accuracy: 89.7457\n",
      "Epoch: 215, Val Loss: 1.0017 Accuracy: 89.7457\n",
      "Epoch: 216, Val Loss: 1.0025 Accuracy: 89.6637\n",
      "Epoch: 217, Val Loss: 1.0039 Accuracy: 89.6637\n",
      "Epoch: 218, Val Loss: 1.0038 Accuracy: 89.5816\n",
      "Epoch: 219, Val Loss: 1.0040 Accuracy: 89.4996\n",
      "Epoch: 220, Val Loss: 1.0021 Accuracy: 89.7457\n",
      "Epoch: 221, Val Loss: 1.0008 Accuracy: 89.9098\n",
      "Epoch: 222, Val Loss: 1.0020 Accuracy: 89.8277\n",
      "Epoch: 223, Val Loss: 1.0015 Accuracy: 89.8277\n",
      "Epoch: 224, Val Loss: 1.0000 Accuracy: 89.9098\n",
      "Epoch: 225, Val Loss: 1.0020 Accuracy: 89.7457\n",
      "Epoch: 226, Val Loss: 1.0020 Accuracy: 89.7457\n",
      "Epoch: 227, Val Loss: 0.9990 Accuracy: 89.9918\n",
      "Epoch: 228, Val Loss: 1.0012 Accuracy: 89.7457\n",
      "Epoch: 229, Val Loss: 1.0004 Accuracy: 89.9098\n",
      "Epoch: 230, Val Loss: 0.9999 Accuracy: 89.8277\n",
      "Epoch: 231, Val Loss: 1.0031 Accuracy: 89.6637\n",
      "Epoch: 232, Val Loss: 1.0024 Accuracy: 89.7457\n",
      "Epoch: 233, Val Loss: 1.0003 Accuracy: 89.9098\n",
      "Epoch: 234, Val Loss: 1.0009 Accuracy: 89.8277\n",
      "Epoch: 235, Val Loss: 0.9994 Accuracy: 89.9098\n",
      "Epoch: 236, Val Loss: 1.0002 Accuracy: 89.9098\n",
      "Epoch: 237, Val Loss: 1.0004 Accuracy: 89.8277\n",
      "Epoch: 238, Val Loss: 1.0004 Accuracy: 89.9098\n",
      "Epoch: 239, Val Loss: 0.9419 Accuracy: 96.3905\n",
      "ok\n",
      "Epoch: 240, Val Loss: 0.9410 Accuracy: 96.5546\n",
      "ok\n",
      "Epoch: 241, Val Loss: 0.9402 Accuracy: 96.7186\n",
      "ok\n",
      "Epoch: 242, Val Loss: 0.9402 Accuracy: 96.4725\n",
      "ok\n",
      "Epoch: 243, Val Loss: 0.9418 Accuracy: 96.3905\n",
      "Epoch: 244, Val Loss: 0.9409 Accuracy: 96.5546\n",
      "Epoch: 245, Val Loss: 0.9387 Accuracy: 96.7186\n",
      "ok\n",
      "Epoch: 246, Val Loss: 0.9393 Accuracy: 96.6366\n",
      "Epoch: 247, Val Loss: 0.9379 Accuracy: 96.8007\n",
      "ok\n",
      "Epoch: 248, Val Loss: 0.9382 Accuracy: 96.7186\n",
      "Epoch: 249, Val Loss: 0.9405 Accuracy: 96.4725\n",
      "Epoch: 250, Val Loss: 0.9389 Accuracy: 96.7186\n",
      "Epoch: 251, Val Loss: 0.9421 Accuracy: 96.3905\n",
      "Epoch: 252, Val Loss: 0.9380 Accuracy: 96.8007\n",
      "Epoch: 253, Val Loss: 0.9391 Accuracy: 96.5546\n",
      "Epoch: 254, Val Loss: 0.9383 Accuracy: 96.7186\n",
      "Epoch: 255, Val Loss: 0.9395 Accuracy: 96.5546\n",
      "Epoch: 256, Val Loss: 0.9391 Accuracy: 96.6366\n",
      "Epoch: 257, Val Loss: 0.9387 Accuracy: 96.6366\n",
      "Epoch: 258, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "ok\n",
      "Epoch: 259, Val Loss: 0.9380 Accuracy: 96.7186\n",
      "Epoch: 260, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 261, Val Loss: 0.9379 Accuracy: 96.7186\n",
      "Epoch: 262, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 263, Val Loss: 0.9386 Accuracy: 96.7186\n",
      "Epoch: 264, Val Loss: 0.9369 Accuracy: 96.8007\n",
      "ok\n",
      "Epoch: 265, Val Loss: 0.9368 Accuracy: 96.8007\n",
      "ok\n",
      "Epoch: 266, Val Loss: 0.9373 Accuracy: 96.8007\n",
      "Epoch: 267, Val Loss: 0.9375 Accuracy: 96.8827\n",
      "Epoch: 268, Val Loss: 0.9385 Accuracy: 96.7186\n",
      "Epoch: 269, Val Loss: 0.9375 Accuracy: 96.8007\n",
      "Epoch: 270, Val Loss: 0.9387 Accuracy: 96.6366\n",
      "Epoch: 271, Val Loss: 0.9388 Accuracy: 96.7186\n",
      "Epoch: 272, Val Loss: 0.9368 Accuracy: 96.8007\n",
      "ok\n",
      "Epoch: 273, Val Loss: 0.9371 Accuracy: 96.8007\n",
      "Epoch: 274, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 275, Val Loss: 0.9381 Accuracy: 96.7186\n",
      "Epoch: 276, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "ok\n",
      "Epoch: 277, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 278, Val Loss: 0.9370 Accuracy: 96.8007\n",
      "Epoch: 279, Val Loss: 0.9371 Accuracy: 96.8827\n",
      "Epoch: 280, Val Loss: 0.9364 Accuracy: 96.8827\n",
      "ok\n",
      "Epoch: 281, Val Loss: 0.9370 Accuracy: 96.8007\n",
      "Epoch: 282, Val Loss: 0.9368 Accuracy: 96.8007\n",
      "Epoch: 283, Val Loss: 0.9371 Accuracy: 96.8007\n",
      "Epoch: 284, Val Loss: 0.9370 Accuracy: 96.8007\n",
      "Epoch: 285, Val Loss: 0.9384 Accuracy: 96.7186\n",
      "Epoch: 286, Val Loss: 0.9373 Accuracy: 96.8007\n",
      "Epoch: 287, Val Loss: 0.9378 Accuracy: 96.7186\n",
      "Epoch: 288, Val Loss: 0.9386 Accuracy: 96.5546\n",
      "Epoch: 289, Val Loss: 0.9382 Accuracy: 96.6366\n",
      "Epoch: 290, Val Loss: 0.9388 Accuracy: 96.5546\n",
      "Epoch: 291, Val Loss: 0.9386 Accuracy: 96.6366\n",
      "Epoch: 292, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 293, Val Loss: 0.9373 Accuracy: 96.8007\n",
      "Epoch: 294, Val Loss: 0.9374 Accuracy: 96.8007\n",
      "Epoch: 295, Val Loss: 0.9366 Accuracy: 96.8827\n",
      "Epoch: 296, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 297, Val Loss: 0.9389 Accuracy: 96.6366\n",
      "Epoch: 298, Val Loss: 0.9378 Accuracy: 96.7186\n",
      "Epoch: 299, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 300, Val Loss: 0.9372 Accuracy: 96.8007\n",
      "Epoch: 301, Val Loss: 0.9372 Accuracy: 96.8007\n",
      "Epoch: 302, Val Loss: 0.9360 Accuracy: 96.8827\n",
      "ok\n",
      "Epoch: 303, Val Loss: 0.9370 Accuracy: 96.8827\n",
      "Epoch: 304, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 305, Val Loss: 0.9369 Accuracy: 96.8007\n",
      "Epoch: 306, Val Loss: 0.9368 Accuracy: 96.7186\n",
      "Epoch: 307, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 308, Val Loss: 0.9413 Accuracy: 96.3084\n",
      "Epoch: 309, Val Loss: 0.9370 Accuracy: 96.8007\n",
      "Epoch: 310, Val Loss: 0.9372 Accuracy: 96.8007\n",
      "Epoch: 311, Val Loss: 0.9379 Accuracy: 96.7186\n",
      "Epoch: 312, Val Loss: 0.9377 Accuracy: 96.7186\n",
      "Epoch: 313, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 314, Val Loss: 0.9371 Accuracy: 96.8007\n",
      "Epoch: 315, Val Loss: 0.9377 Accuracy: 96.7186\n",
      "Epoch: 316, Val Loss: 0.9383 Accuracy: 96.6366\n",
      "Epoch: 317, Val Loss: 0.9393 Accuracy: 96.5546\n",
      "Epoch: 318, Val Loss: 0.9388 Accuracy: 96.5546\n",
      "Epoch: 319, Val Loss: 0.9387 Accuracy: 96.6366\n",
      "Epoch: 320, Val Loss: 0.9387 Accuracy: 96.5546\n",
      "Epoch: 321, Val Loss: 0.9379 Accuracy: 96.7186\n",
      "Epoch: 322, Val Loss: 0.9387 Accuracy: 96.6366\n",
      "Epoch: 323, Val Loss: 0.9403 Accuracy: 96.4725\n",
      "Epoch: 324, Val Loss: 0.9390 Accuracy: 96.5546\n",
      "Epoch: 325, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 326, Val Loss: 0.9393 Accuracy: 96.6366\n",
      "Epoch: 327, Val Loss: 0.9391 Accuracy: 96.6366\n",
      "Epoch: 328, Val Loss: 0.9383 Accuracy: 96.6366\n",
      "Epoch: 329, Val Loss: 0.9389 Accuracy: 96.6366\n",
      "Epoch: 330, Val Loss: 0.9378 Accuracy: 96.7186\n",
      "Epoch: 331, Val Loss: 0.9385 Accuracy: 96.5546\n",
      "Epoch: 332, Val Loss: 0.9395 Accuracy: 96.5546\n",
      "Epoch: 333, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 334, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 335, Val Loss: 0.9387 Accuracy: 96.6366\n",
      "Epoch: 336, Val Loss: 0.9370 Accuracy: 96.8007\n",
      "Epoch: 337, Val Loss: 0.9389 Accuracy: 96.6366\n",
      "Epoch: 338, Val Loss: 0.9388 Accuracy: 96.6366\n",
      "Epoch: 339, Val Loss: 0.9384 Accuracy: 96.6366\n",
      "Epoch: 340, Val Loss: 0.9373 Accuracy: 96.8007\n",
      "Epoch: 341, Val Loss: 0.9396 Accuracy: 96.4725\n",
      "Epoch: 342, Val Loss: 0.9404 Accuracy: 96.4725\n",
      "Epoch: 343, Val Loss: 0.9382 Accuracy: 96.6366\n",
      "Epoch: 344, Val Loss: 0.9363 Accuracy: 96.8827\n",
      "Epoch: 345, Val Loss: 0.9361 Accuracy: 96.8827\n",
      "Epoch: 346, Val Loss: 0.9369 Accuracy: 96.8007\n",
      "Epoch: 347, Val Loss: 0.9357 Accuracy: 96.9647\n",
      "ok\n",
      "Epoch: 348, Val Loss: 0.9383 Accuracy: 96.7186\n",
      "Epoch: 349, Val Loss: 0.9389 Accuracy: 96.5546\n",
      "Epoch: 350, Val Loss: 0.9388 Accuracy: 96.6366\n",
      "Epoch: 351, Val Loss: 0.9371 Accuracy: 96.8007\n",
      "Epoch: 352, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 353, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 354, Val Loss: 0.9362 Accuracy: 96.8827\n",
      "Epoch: 355, Val Loss: 0.9383 Accuracy: 96.6366\n",
      "Epoch: 356, Val Loss: 0.9388 Accuracy: 96.6366\n",
      "Epoch: 357, Val Loss: 0.9389 Accuracy: 96.4725\n",
      "Epoch: 358, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 359, Val Loss: 0.9401 Accuracy: 96.4725\n",
      "Epoch: 360, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 361, Val Loss: 0.9383 Accuracy: 96.6366\n",
      "Epoch: 362, Val Loss: 0.9391 Accuracy: 96.5546\n",
      "Epoch: 363, Val Loss: 0.9405 Accuracy: 96.3905\n",
      "Epoch: 364, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 365, Val Loss: 0.9382 Accuracy: 96.7186\n",
      "Epoch: 366, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 367, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 368, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 369, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 370, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 371, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 372, Val Loss: 0.9386 Accuracy: 96.5546\n",
      "Epoch: 373, Val Loss: 0.9384 Accuracy: 96.6366\n",
      "Epoch: 374, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 375, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 376, Val Loss: 0.9383 Accuracy: 96.6366\n",
      "Epoch: 377, Val Loss: 0.9385 Accuracy: 96.5546\n",
      "Epoch: 378, Val Loss: 0.9361 Accuracy: 96.8007\n",
      "Epoch: 379, Val Loss: 0.9383 Accuracy: 96.6366\n",
      "Epoch: 380, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 381, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 382, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 383, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 384, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 385, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 386, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 387, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 388, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 389, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 390, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 391, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 392, Val Loss: 0.9378 Accuracy: 96.7186\n",
      "Epoch: 393, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 394, Val Loss: 0.9381 Accuracy: 96.6366\n",
      "Epoch: 395, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 396, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 397, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 398, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 399, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 400, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 401, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 402, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 403, Val Loss: 0.9370 Accuracy: 96.8007\n",
      "Epoch: 404, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 405, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 406, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 407, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 408, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 409, Val Loss: 0.9369 Accuracy: 96.8007\n",
      "Epoch: 410, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 411, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 412, Val Loss: 0.9386 Accuracy: 96.6366\n",
      "Epoch: 413, Val Loss: 0.9371 Accuracy: 96.8007\n",
      "Epoch: 414, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 415, Val Loss: 0.9369 Accuracy: 96.8007\n",
      "Epoch: 416, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 417, Val Loss: 0.9360 Accuracy: 96.8827\n",
      "Epoch: 418, Val Loss: 0.9359 Accuracy: 96.8827\n",
      "Epoch: 419, Val Loss: 0.9356 Accuracy: 96.8827\n",
      "ok\n",
      "Epoch: 420, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 421, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 422, Val Loss: 0.9377 Accuracy: 96.7186\n",
      "Epoch: 423, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 424, Val Loss: 0.9384 Accuracy: 96.6366\n",
      "Epoch: 425, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 426, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 427, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 428, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 429, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 430, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 431, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 432, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 433, Val Loss: 0.9371 Accuracy: 96.6366\n",
      "Epoch: 434, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 435, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 436, Val Loss: 0.9385 Accuracy: 96.6366\n",
      "Epoch: 437, Val Loss: 0.9369 Accuracy: 96.8007\n",
      "Epoch: 438, Val Loss: 0.9389 Accuracy: 96.5546\n",
      "Epoch: 439, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 440, Val Loss: 0.9383 Accuracy: 96.6366\n",
      "Epoch: 441, Val Loss: 0.9386 Accuracy: 96.5546\n",
      "Epoch: 442, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 443, Val Loss: 0.9394 Accuracy: 96.5546\n",
      "Epoch: 444, Val Loss: 0.9368 Accuracy: 96.8007\n",
      "Epoch: 445, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 446, Val Loss: 0.9379 Accuracy: 96.7186\n",
      "Epoch: 447, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 448, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 449, Val Loss: 0.9387 Accuracy: 96.5546\n",
      "Epoch: 450, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 451, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 452, Val Loss: 0.9378 Accuracy: 96.7186\n",
      "Epoch: 453, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 454, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 455, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 456, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 457, Val Loss: 0.9400 Accuracy: 96.3905\n",
      "Epoch: 458, Val Loss: 0.9366 Accuracy: 96.7186\n",
      "Epoch: 459, Val Loss: 0.9373 Accuracy: 96.6366\n",
      "Epoch: 460, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 461, Val Loss: 0.9358 Accuracy: 96.8827\n",
      "Epoch: 462, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 463, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 464, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 465, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 466, Val Loss: 0.9368 Accuracy: 96.7186\n",
      "Epoch: 467, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 468, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 469, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 470, Val Loss: 0.9369 Accuracy: 96.8007\n",
      "Epoch: 471, Val Loss: 0.9381 Accuracy: 96.6366\n",
      "Epoch: 472, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 473, Val Loss: 0.9386 Accuracy: 96.5546\n",
      "Epoch: 474, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 475, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 476, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 477, Val Loss: 0.9374 Accuracy: 96.8007\n",
      "Epoch: 478, Val Loss: 0.9368 Accuracy: 96.8007\n",
      "Epoch: 479, Val Loss: 0.9359 Accuracy: 96.8007\n",
      "Epoch: 480, Val Loss: 0.9359 Accuracy: 96.8827\n",
      "Epoch: 481, Val Loss: 0.9360 Accuracy: 96.8827\n",
      "Epoch: 482, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 483, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 484, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 485, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 486, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 487, Val Loss: 0.9377 Accuracy: 96.7186\n",
      "Epoch: 488, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 489, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 490, Val Loss: 0.9388 Accuracy: 96.8007\n",
      "Epoch: 491, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 492, Val Loss: 0.9359 Accuracy: 96.8827\n",
      "Epoch: 493, Val Loss: 0.9358 Accuracy: 96.8827\n",
      "Epoch: 494, Val Loss: 0.9359 Accuracy: 96.8827\n",
      "Epoch: 495, Val Loss: 0.9356 Accuracy: 96.8827\n",
      "ok\n",
      "Epoch: 496, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 497, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 498, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 499, Val Loss: 0.9370 Accuracy: 96.8007\n",
      "Epoch: 500, Val Loss: 0.9361 Accuracy: 96.8827\n",
      "Epoch: 501, Val Loss: 0.9356 Accuracy: 96.8827\n",
      "Epoch: 502, Val Loss: 0.9350 Accuracy: 96.9647\n",
      "ok\n",
      "Epoch: 503, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 504, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 505, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 506, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 507, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 508, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 509, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 510, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 511, Val Loss: 0.9383 Accuracy: 96.5546\n",
      "Epoch: 512, Val Loss: 0.9356 Accuracy: 96.8827\n",
      "Epoch: 513, Val Loss: 0.9354 Accuracy: 96.8827\n",
      "Epoch: 514, Val Loss: 0.9356 Accuracy: 96.8827\n",
      "Epoch: 515, Val Loss: 0.9358 Accuracy: 96.8827\n",
      "Epoch: 516, Val Loss: 0.9382 Accuracy: 96.6366\n",
      "Epoch: 517, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 518, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 519, Val Loss: 0.9356 Accuracy: 96.8827\n",
      "Epoch: 520, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 521, Val Loss: 0.9372 Accuracy: 96.8007\n",
      "Epoch: 522, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 523, Val Loss: 0.9354 Accuracy: 96.8827\n",
      "Epoch: 524, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 525, Val Loss: 0.9364 Accuracy: 96.8827\n",
      "Epoch: 526, Val Loss: 0.9382 Accuracy: 96.6366\n",
      "Epoch: 527, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 528, Val Loss: 0.9368 Accuracy: 96.7186\n",
      "Epoch: 529, Val Loss: 0.9366 Accuracy: 96.7186\n",
      "Epoch: 530, Val Loss: 0.9354 Accuracy: 96.8827\n",
      "Epoch: 531, Val Loss: 0.9361 Accuracy: 96.8007\n",
      "Epoch: 532, Val Loss: 0.9360 Accuracy: 96.8827\n",
      "Epoch: 533, Val Loss: 0.9359 Accuracy: 96.8007\n",
      "Epoch: 534, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 535, Val Loss: 0.9361 Accuracy: 96.8007\n",
      "Epoch: 536, Val Loss: 0.9359 Accuracy: 96.8827\n",
      "Epoch: 537, Val Loss: 0.9361 Accuracy: 96.8827\n",
      "Epoch: 538, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 539, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 540, Val Loss: 0.9407 Accuracy: 96.3084\n",
      "Epoch: 541, Val Loss: 0.9393 Accuracy: 96.4725\n",
      "Epoch: 542, Val Loss: 0.9389 Accuracy: 96.5546\n",
      "Epoch: 543, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 544, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 545, Val Loss: 0.9370 Accuracy: 96.8007\n",
      "Epoch: 546, Val Loss: 0.9369 Accuracy: 96.8007\n",
      "Epoch: 547, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 548, Val Loss: 0.9357 Accuracy: 96.8827\n",
      "Epoch: 549, Val Loss: 0.9394 Accuracy: 96.4725\n",
      "Epoch: 550, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 551, Val Loss: 0.9383 Accuracy: 96.6366\n",
      "Epoch: 552, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 553, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 554, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 555, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 556, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 557, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 558, Val Loss: 0.9396 Accuracy: 96.5546\n",
      "Epoch: 559, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 560, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 561, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 562, Val Loss: 0.9368 Accuracy: 96.7186\n",
      "Epoch: 563, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 564, Val Loss: 0.9358 Accuracy: 96.8827\n",
      "Epoch: 565, Val Loss: 0.9368 Accuracy: 96.7186\n",
      "Epoch: 566, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 567, Val Loss: 0.9388 Accuracy: 96.6366\n",
      "Epoch: 568, Val Loss: 0.9387 Accuracy: 96.5546\n",
      "Epoch: 569, Val Loss: 0.9372 Accuracy: 96.8007\n",
      "Epoch: 570, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 571, Val Loss: 0.9365 Accuracy: 96.7186\n",
      "Epoch: 572, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 573, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 574, Val Loss: 0.9352 Accuracy: 96.9647\n",
      "Epoch: 575, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 576, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 577, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 578, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 579, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 580, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 581, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 582, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 583, Val Loss: 0.9357 Accuracy: 96.8007\n",
      "Epoch: 584, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 585, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 586, Val Loss: 0.9348 Accuracy: 96.9647\n",
      "ok\n",
      "Epoch: 587, Val Loss: 0.9358 Accuracy: 96.8007\n",
      "Epoch: 588, Val Loss: 0.9357 Accuracy: 96.8827\n",
      "Epoch: 589, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 590, Val Loss: 0.9354 Accuracy: 96.8827\n",
      "Epoch: 591, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 592, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 593, Val Loss: 0.9368 Accuracy: 96.7186\n",
      "Epoch: 594, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 595, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 596, Val Loss: 0.9350 Accuracy: 96.9647\n",
      "Epoch: 597, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 598, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 599, Val Loss: 0.9388 Accuracy: 96.5546\n",
      "Epoch: 600, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 601, Val Loss: 0.9360 Accuracy: 96.8007\n",
      "Epoch: 602, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 603, Val Loss: 0.9375 Accuracy: 96.6366\n",
      "Epoch: 604, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 605, Val Loss: 0.9355 Accuracy: 96.9647\n",
      "Epoch: 606, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 607, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 608, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 609, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 610, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 611, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 612, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 613, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 614, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 615, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 616, Val Loss: 0.9357 Accuracy: 96.8827\n",
      "Epoch: 617, Val Loss: 0.9350 Accuracy: 96.9647\n",
      "Epoch: 618, Val Loss: 0.9358 Accuracy: 96.8007\n",
      "Epoch: 619, Val Loss: 0.9351 Accuracy: 96.9647\n",
      "Epoch: 620, Val Loss: 0.9353 Accuracy: 96.8827\n",
      "Epoch: 621, Val Loss: 0.9367 Accuracy: 96.7186\n",
      "Epoch: 622, Val Loss: 0.9353 Accuracy: 96.8827\n",
      "Epoch: 623, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 624, Val Loss: 0.9362 Accuracy: 96.7186\n",
      "Epoch: 625, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 626, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 627, Val Loss: 0.9377 Accuracy: 96.7186\n",
      "Epoch: 628, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 629, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 630, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 631, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 632, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 633, Val Loss: 0.9386 Accuracy: 96.5546\n",
      "Epoch: 634, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 635, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 636, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 637, Val Loss: 0.9361 Accuracy: 96.8007\n",
      "Epoch: 638, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 639, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 640, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 641, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 642, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 643, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 644, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 645, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 646, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 647, Val Loss: 0.9368 Accuracy: 96.7186\n",
      "Epoch: 648, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 649, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 650, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 651, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 652, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 653, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 654, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 655, Val Loss: 0.9373 Accuracy: 96.8007\n",
      "Epoch: 656, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 657, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 658, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 659, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 660, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 661, Val Loss: 0.9355 Accuracy: 96.8827\n",
      "Epoch: 662, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 663, Val Loss: 0.9370 Accuracy: 96.8007\n",
      "Epoch: 664, Val Loss: 0.9368 Accuracy: 96.7186\n",
      "Epoch: 665, Val Loss: 0.9367 Accuracy: 96.7186\n",
      "Epoch: 666, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 667, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 668, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 669, Val Loss: 0.9382 Accuracy: 96.6366\n",
      "Epoch: 670, Val Loss: 0.9386 Accuracy: 96.5546\n",
      "Epoch: 671, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 672, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 673, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 674, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 675, Val Loss: 0.9374 Accuracy: 96.6366\n",
      "Epoch: 676, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 677, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 678, Val Loss: 0.9353 Accuracy: 96.8827\n",
      "Epoch: 679, Val Loss: 0.9381 Accuracy: 96.6366\n",
      "Epoch: 680, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 681, Val Loss: 0.9361 Accuracy: 96.8827\n",
      "Epoch: 682, Val Loss: 0.9360 Accuracy: 96.8827\n",
      "Epoch: 683, Val Loss: 0.9361 Accuracy: 96.8007\n",
      "Epoch: 684, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 685, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 686, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 687, Val Loss: 0.9359 Accuracy: 96.8827\n",
      "Epoch: 688, Val Loss: 0.9367 Accuracy: 96.7186\n",
      "Epoch: 689, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 690, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 691, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 692, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 693, Val Loss: 0.9367 Accuracy: 96.7186\n",
      "Epoch: 694, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 695, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 696, Val Loss: 0.9355 Accuracy: 96.8827\n",
      "Epoch: 697, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 698, Val Loss: 0.9382 Accuracy: 96.6366\n",
      "Epoch: 699, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 700, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 701, Val Loss: 0.9357 Accuracy: 96.8827\n",
      "Epoch: 702, Val Loss: 0.9356 Accuracy: 96.8827\n",
      "Epoch: 703, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 704, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 705, Val Loss: 0.9434 Accuracy: 96.1444\n",
      "Epoch: 706, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 707, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 708, Val Loss: 0.9375 Accuracy: 96.6366\n",
      "Epoch: 709, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 710, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 711, Val Loss: 0.9368 Accuracy: 96.8007\n",
      "Epoch: 712, Val Loss: 0.9384 Accuracy: 96.5546\n",
      "Epoch: 713, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 714, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 715, Val Loss: 0.9382 Accuracy: 96.6366\n",
      "Epoch: 716, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 717, Val Loss: 0.9355 Accuracy: 96.8827\n",
      "Epoch: 718, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 719, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 720, Val Loss: 0.9367 Accuracy: 96.7186\n",
      "Epoch: 721, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 722, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 723, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 724, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 725, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 726, Val Loss: 0.9347 Accuracy: 96.9647\n",
      "ok\n",
      "Epoch: 727, Val Loss: 0.9372 Accuracy: 96.8007\n",
      "Epoch: 728, Val Loss: 0.9366 Accuracy: 96.7186\n",
      "Epoch: 729, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 730, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 731, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 732, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 733, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 734, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 735, Val Loss: 0.9374 Accuracy: 96.6366\n",
      "Epoch: 736, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 737, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 738, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 739, Val Loss: 0.9380 Accuracy: 96.7186\n",
      "Epoch: 740, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 741, Val Loss: 0.9355 Accuracy: 96.8827\n",
      "Epoch: 742, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 743, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 744, Val Loss: 0.9355 Accuracy: 96.8827\n",
      "Epoch: 745, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 746, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 747, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 748, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 749, Val Loss: 0.9389 Accuracy: 96.5546\n",
      "Epoch: 750, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 751, Val Loss: 0.9387 Accuracy: 96.4725\n",
      "Epoch: 752, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 753, Val Loss: 0.9379 Accuracy: 96.7186\n",
      "Epoch: 754, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 755, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 756, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 757, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 758, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 759, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 760, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 761, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 762, Val Loss: 0.9361 Accuracy: 96.8007\n",
      "Epoch: 763, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 764, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 765, Val Loss: 0.9349 Accuracy: 96.9647\n",
      "Epoch: 766, Val Loss: 0.9358 Accuracy: 96.8827\n",
      "Epoch: 767, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 768, Val Loss: 0.9356 Accuracy: 96.8827\n",
      "Epoch: 769, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 770, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 771, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 772, Val Loss: 0.9357 Accuracy: 96.8827\n",
      "Epoch: 773, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 774, Val Loss: 0.9350 Accuracy: 96.9647\n",
      "Epoch: 775, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 776, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 777, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 778, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 779, Val Loss: 0.9381 Accuracy: 96.6366\n",
      "Epoch: 780, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 781, Val Loss: 0.9389 Accuracy: 96.4725\n",
      "Epoch: 782, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 783, Val Loss: 0.9396 Accuracy: 96.4725\n",
      "Epoch: 784, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 785, Val Loss: 0.9374 Accuracy: 96.6366\n",
      "Epoch: 786, Val Loss: 0.9392 Accuracy: 96.4725\n",
      "Epoch: 787, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 788, Val Loss: 0.9378 Accuracy: 96.7186\n",
      "Epoch: 789, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 790, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 791, Val Loss: 0.9382 Accuracy: 96.6366\n",
      "Epoch: 792, Val Loss: 0.9388 Accuracy: 96.5546\n",
      "Epoch: 793, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 794, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 795, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 796, Val Loss: 0.9374 Accuracy: 96.6366\n",
      "Epoch: 797, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 798, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 799, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 800, Val Loss: 0.9383 Accuracy: 96.5546\n",
      "Epoch: 801, Val Loss: 0.9414 Accuracy: 96.3905\n",
      "Epoch: 802, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 803, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 804, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 805, Val Loss: 0.9380 Accuracy: 96.5546\n",
      "Epoch: 806, Val Loss: 0.9377 Accuracy: 96.7186\n",
      "Epoch: 807, Val Loss: 0.9375 Accuracy: 96.6366\n",
      "Epoch: 808, Val Loss: 0.9357 Accuracy: 96.8827\n",
      "Epoch: 809, Val Loss: 0.9355 Accuracy: 96.8007\n",
      "Epoch: 810, Val Loss: 0.9359 Accuracy: 96.8007\n",
      "Epoch: 811, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 812, Val Loss: 0.9367 Accuracy: 96.7186\n",
      "Epoch: 813, Val Loss: 0.9372 Accuracy: 96.6366\n",
      "Epoch: 814, Val Loss: 0.9376 Accuracy: 96.5546\n",
      "Epoch: 815, Val Loss: 0.9361 Accuracy: 96.8007\n",
      "Epoch: 816, Val Loss: 0.9381 Accuracy: 96.7186\n",
      "Epoch: 817, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 818, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 819, Val Loss: 0.9387 Accuracy: 96.5546\n",
      "Epoch: 820, Val Loss: 0.9395 Accuracy: 96.3905\n",
      "Epoch: 821, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 822, Val Loss: 0.9375 Accuracy: 96.6366\n",
      "Epoch: 823, Val Loss: 0.9385 Accuracy: 96.5546\n",
      "Epoch: 824, Val Loss: 0.9395 Accuracy: 96.4725\n",
      "Epoch: 825, Val Loss: 0.9381 Accuracy: 96.6366\n",
      "Epoch: 826, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 827, Val Loss: 0.9375 Accuracy: 96.6366\n",
      "Epoch: 828, Val Loss: 0.9386 Accuracy: 96.5546\n",
      "Epoch: 829, Val Loss: 0.9374 Accuracy: 96.6366\n",
      "Epoch: 830, Val Loss: 0.9383 Accuracy: 96.5546\n",
      "Epoch: 831, Val Loss: 0.9381 Accuracy: 96.6366\n",
      "Epoch: 832, Val Loss: 0.9387 Accuracy: 96.5546\n",
      "Epoch: 833, Val Loss: 0.9393 Accuracy: 96.4725\n",
      "Epoch: 834, Val Loss: 0.9384 Accuracy: 96.5546\n",
      "Epoch: 835, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 836, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 837, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 838, Val Loss: 0.9374 Accuracy: 96.6366\n",
      "Epoch: 839, Val Loss: 0.9374 Accuracy: 96.6366\n",
      "Epoch: 840, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 841, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 842, Val Loss: 0.9369 Accuracy: 96.6366\n",
      "Epoch: 843, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 844, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 845, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 846, Val Loss: 0.9361 Accuracy: 96.8007\n",
      "Epoch: 847, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 848, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 849, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 850, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 851, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 852, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 853, Val Loss: 0.9385 Accuracy: 96.5546\n",
      "Epoch: 854, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 855, Val Loss: 0.9368 Accuracy: 96.8007\n",
      "Epoch: 856, Val Loss: 0.9361 Accuracy: 96.8007\n",
      "Epoch: 857, Val Loss: 0.9366 Accuracy: 96.7186\n",
      "Epoch: 858, Val Loss: 0.9371 Accuracy: 96.8007\n",
      "Epoch: 859, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 860, Val Loss: 0.9382 Accuracy: 96.6366\n",
      "Epoch: 861, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 862, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 863, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 864, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 865, Val Loss: 0.9382 Accuracy: 96.6366\n",
      "Epoch: 866, Val Loss: 0.9381 Accuracy: 96.6366\n",
      "Epoch: 867, Val Loss: 0.9387 Accuracy: 96.5546\n",
      "Epoch: 868, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 869, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 870, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 871, Val Loss: 0.9359 Accuracy: 96.8007\n",
      "Epoch: 872, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 873, Val Loss: 0.9367 Accuracy: 96.7186\n",
      "Epoch: 874, Val Loss: 0.9397 Accuracy: 96.4725\n",
      "Epoch: 875, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 876, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 877, Val Loss: 0.9373 Accuracy: 96.6366\n",
      "Epoch: 878, Val Loss: 0.9383 Accuracy: 96.5546\n",
      "Epoch: 879, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 880, Val Loss: 0.9366 Accuracy: 96.7186\n",
      "Epoch: 881, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 882, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 883, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 884, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 885, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 886, Val Loss: 0.9384 Accuracy: 96.5546\n",
      "Epoch: 887, Val Loss: 0.9375 Accuracy: 96.6366\n",
      "Epoch: 888, Val Loss: 0.9376 Accuracy: 96.7186\n",
      "Epoch: 889, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 890, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 891, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 892, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 893, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 894, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 895, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 896, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 897, Val Loss: 0.9355 Accuracy: 96.8827\n",
      "Epoch: 898, Val Loss: 0.9384 Accuracy: 96.4725\n",
      "Epoch: 899, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 900, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 901, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 902, Val Loss: 0.9356 Accuracy: 96.8827\n",
      "Epoch: 903, Val Loss: 0.9349 Accuracy: 96.9647\n",
      "Epoch: 904, Val Loss: 0.9375 Accuracy: 96.6366\n",
      "Epoch: 905, Val Loss: 0.9461 Accuracy: 95.7342\n",
      "Epoch: 906, Val Loss: 0.9468 Accuracy: 95.7342\n",
      "Epoch: 907, Val Loss: 0.9377 Accuracy: 96.7186\n",
      "Epoch: 908, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 909, Val Loss: 0.9354 Accuracy: 96.8827\n",
      "Epoch: 910, Val Loss: 0.9354 Accuracy: 96.8827\n",
      "Epoch: 911, Val Loss: 0.9358 Accuracy: 96.8007\n",
      "Epoch: 912, Val Loss: 0.9354 Accuracy: 96.8827\n",
      "Epoch: 913, Val Loss: 0.9354 Accuracy: 96.8827\n",
      "Epoch: 914, Val Loss: 0.9357 Accuracy: 96.8827\n",
      "Epoch: 915, Val Loss: 0.9356 Accuracy: 96.8827\n",
      "Epoch: 916, Val Loss: 0.9359 Accuracy: 96.8827\n",
      "Epoch: 917, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 918, Val Loss: 0.9388 Accuracy: 96.4725\n",
      "Epoch: 919, Val Loss: 0.9387 Accuracy: 96.5546\n",
      "Epoch: 920, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 921, Val Loss: 0.9402 Accuracy: 96.3905\n",
      "Epoch: 922, Val Loss: 0.9370 Accuracy: 96.6366\n",
      "Epoch: 923, Val Loss: 0.9348 Accuracy: 96.9647\n",
      "Epoch: 924, Val Loss: 0.9357 Accuracy: 96.8007\n",
      "Epoch: 925, Val Loss: 0.9353 Accuracy: 96.8827\n",
      "Epoch: 926, Val Loss: 0.9347 Accuracy: 96.9647\n",
      "Epoch: 927, Val Loss: 0.9348 Accuracy: 96.9647\n",
      "Epoch: 928, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 929, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 930, Val Loss: 0.9354 Accuracy: 96.8827\n",
      "Epoch: 931, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 932, Val Loss: 0.9361 Accuracy: 96.8827\n",
      "Epoch: 933, Val Loss: 0.9367 Accuracy: 96.8007\n",
      "Epoch: 934, Val Loss: 0.9358 Accuracy: 96.8007\n",
      "Epoch: 935, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 936, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 937, Val Loss: 0.9363 Accuracy: 96.7186\n",
      "Epoch: 938, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 939, Val Loss: 0.9381 Accuracy: 96.6366\n",
      "Epoch: 940, Val Loss: 0.9394 Accuracy: 96.4725\n",
      "Epoch: 941, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 942, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 943, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 944, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 945, Val Loss: 0.9373 Accuracy: 96.6366\n",
      "Epoch: 946, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 947, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 948, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 949, Val Loss: 0.9390 Accuracy: 96.4725\n",
      "Epoch: 950, Val Loss: 0.9385 Accuracy: 96.5546\n",
      "Epoch: 951, Val Loss: 0.9374 Accuracy: 96.6366\n",
      "Epoch: 952, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 953, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 954, Val Loss: 0.9377 Accuracy: 96.6366\n",
      "Epoch: 955, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 956, Val Loss: 0.9375 Accuracy: 96.6366\n",
      "Epoch: 957, Val Loss: 0.9372 Accuracy: 96.7186\n",
      "Epoch: 958, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 959, Val Loss: 0.9384 Accuracy: 96.5546\n",
      "Epoch: 960, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 961, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 962, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 963, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 964, Val Loss: 0.9361 Accuracy: 96.8007\n",
      "Epoch: 965, Val Loss: 0.9362 Accuracy: 96.8007\n",
      "Epoch: 966, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 967, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 968, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 969, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 970, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 971, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 972, Val Loss: 0.9373 Accuracy: 96.7186\n",
      "Epoch: 973, Val Loss: 0.9365 Accuracy: 96.8007\n",
      "Epoch: 974, Val Loss: 0.9370 Accuracy: 96.8007\n",
      "Epoch: 975, Val Loss: 0.9376 Accuracy: 96.6366\n",
      "Epoch: 976, Val Loss: 0.9379 Accuracy: 96.6366\n",
      "Epoch: 977, Val Loss: 0.9393 Accuracy: 96.4725\n",
      "Epoch: 978, Val Loss: 0.9382 Accuracy: 96.5546\n",
      "Epoch: 979, Val Loss: 0.9371 Accuracy: 96.7186\n",
      "Epoch: 980, Val Loss: 0.9364 Accuracy: 96.8007\n",
      "Epoch: 981, Val Loss: 0.9386 Accuracy: 96.5546\n",
      "Epoch: 982, Val Loss: 0.9385 Accuracy: 96.5546\n",
      "Epoch: 983, Val Loss: 0.9394 Accuracy: 96.4725\n",
      "Epoch: 984, Val Loss: 0.9374 Accuracy: 96.7186\n",
      "Epoch: 985, Val Loss: 0.9388 Accuracy: 96.5546\n",
      "Epoch: 986, Val Loss: 0.9378 Accuracy: 96.6366\n",
      "Epoch: 987, Val Loss: 0.9368 Accuracy: 96.7186\n",
      "Epoch: 988, Val Loss: 0.9346 Accuracy: 96.9647\n",
      "ok\n",
      "Epoch: 989, Val Loss: 0.9347 Accuracy: 96.9647\n",
      "Epoch: 990, Val Loss: 0.9380 Accuracy: 96.6366\n",
      "Epoch: 991, Val Loss: 0.9381 Accuracy: 96.6366\n",
      "Epoch: 992, Val Loss: 0.9366 Accuracy: 96.7186\n",
      "Epoch: 993, Val Loss: 0.9363 Accuracy: 96.8007\n",
      "Epoch: 994, Val Loss: 0.9358 Accuracy: 96.8827\n",
      "Epoch: 995, Val Loss: 0.9370 Accuracy: 96.7186\n",
      "Epoch: 996, Val Loss: 0.9366 Accuracy: 96.8007\n",
      "Epoch: 997, Val Loss: 0.9375 Accuracy: 96.7186\n",
      "Epoch: 998, Val Loss: 0.9369 Accuracy: 96.7186\n",
      "Epoch: 999, Val Loss: 0.9375 Accuracy: 96.6366\n",
      "Epoch: 1000, Val Loss: 0.9381 Accuracy: 96.6366\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "counter = 0 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    myModel.train()\n",
    "    for x, y in train_loader:\n",
    "        output = myModel(x)\n",
    "        loss = criterion(output,y.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    myModel.eval()\n",
    "    val_loss = 0.00\n",
    "    total = 0\n",
    "    correct = 0.00 \n",
    "    with torch.no_grad():\n",
    "        for x,y in test_loader:\n",
    "            output = myModel(x)\n",
    "            _, predicted = torch.max(output.data,1)\n",
    "            loss = criterion(output,y.long())\n",
    "            val_loss += loss.item()\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    val_loss /= len(test_loader)\n",
    "    accuracy = (100*correct)/total\n",
    "    print(f'Epoch: {epoch+1}, Val Loss: {val_loss:.4f} Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(myModel.state_dict(),'best_model.pth')\n",
    "        print('ok')\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:51:35.659047Z",
     "iopub.status.busy": "2025-04-07T08:51:35.658709Z",
     "iopub.status.idle": "2025-04-07T08:51:35.676620Z",
     "shell.execute_reply": "2025-04-07T08:51:35.675805Z",
     "shell.execute_reply.started": "2025-04-07T08:51:35.659017Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'0.weight': tensor([[-9.2397e-02,  2.9384e-02,  3.2992e-02,  4.5138e-01,  1.7233e-01,\n",
      "          6.6353e-02,  1.4985e-01, -6.1230e-01,  1.2495e-01, -6.4370e-01,\n",
      "         -1.3329e-01,  3.9762e-01,  8.7608e-02, -9.1438e-03, -3.8680e-02,\n",
      "         -8.5716e-02,  1.9608e-02, -1.3613e-01, -2.9353e-03,  2.1105e-01,\n",
      "          4.5788e-02, -3.8452e-01,  2.9907e-02,  2.0438e-02, -1.4651e-01,\n",
      "          5.8894e-01, -1.6818e-01, -1.6222e-01,  2.8279e-02, -2.9030e-01,\n",
      "         -2.6488e-01,  1.9776e-01, -1.4271e-01,  1.1305e+00,  9.1884e-03,\n",
      "         -5.9085e-01,  1.0283e-01, -3.6207e-01, -6.1441e-02,  4.1042e-01,\n",
      "         -3.9726e-01,  1.1168e+00],\n",
      "        [-2.8184e-02, -1.3310e-01,  5.4186e-01,  4.6994e-02,  3.6728e-01,\n",
      "         -2.0945e-01,  1.0815e-01, -9.5666e-02, -1.4874e-01, -1.1126e-01,\n",
      "          5.6141e-01,  1.7558e-01,  5.4185e-02, -5.0412e-02, -2.4319e-01,\n",
      "          1.8259e-01, -7.6494e-01,  5.8106e-01,  3.6904e-01,  1.2940e-01,\n",
      "          1.6873e-01, -2.1426e-01,  3.0097e-01, -4.0048e-01,  4.9785e-01,\n",
      "         -3.6770e-01,  1.4277e-01, -9.1216e-02,  1.2288e-01, -5.5502e-02,\n",
      "          4.8450e-01, -2.2728e-01,  6.0149e-01, -2.4503e-01, -5.3624e-02,\n",
      "         -1.5104e-01, -1.7192e-01,  2.7291e-01,  2.6779e-02,  2.3348e-02,\n",
      "          8.2797e-02, -2.9204e-02],\n",
      "        [ 1.5425e-02,  1.7836e-02,  4.0489e-01, -4.6477e-02,  3.2389e-01,\n",
      "         -3.0102e-01,  3.9843e-01, -2.7309e-01,  1.0477e-01, -1.1731e-01,\n",
      "          6.0502e-01, -8.6227e-02,  1.7433e-01, -6.8690e-02, -5.0874e-01,\n",
      "          2.4848e-02, -7.7500e-01,  5.0084e-01,  5.8739e-01, -5.5192e-02,\n",
      "          2.5829e-01, -1.1362e-01, -6.1817e-02, -2.1880e-01,  1.1310e-02,\n",
      "          1.1937e-01,  2.1941e-01, -1.4599e-01,  7.1679e-02, -1.3612e-01,\n",
      "         -1.0007e-01, -5.3202e-02,  1.1965e-02,  1.5289e-01,  2.9284e-02,\n",
      "         -1.3775e-01,  2.7644e-01,  3.1022e-05,  6.5554e-02, -8.7771e-02,\n",
      "         -5.9630e-02,  2.5805e-01],\n",
      "        [-6.4011e-02, -9.7975e-02,  5.0777e-01,  2.6069e-01,  4.8199e-01,\n",
      "          1.6457e-01,  5.8577e-01,  3.5655e-01,  4.1216e-01,  2.1436e-02,\n",
      "          4.8099e-01,  5.6181e-01,  7.8136e-02,  2.8337e-01, -2.9387e-01,\n",
      "         -1.4820e-01, -7.8452e-01, -2.7029e-01,  2.9042e-01,  1.7328e-01,\n",
      "          1.8952e-01,  1.9785e-01,  9.3797e-02, -1.8216e-01, -9.4838e-03,\n",
      "         -4.0995e-02,  1.2334e-01, -2.1991e-02,  1.9605e-01, -1.1007e-01,\n",
      "          6.7587e-02, -6.2217e-02, -1.8270e-01,  1.5919e-01,  1.8893e-02,\n",
      "         -1.2882e-01, -1.8004e-02, -1.5086e-01, -1.3982e-01, -9.4152e-02,\n",
      "         -1.5533e-01,  2.5182e-02],\n",
      "        [ 1.2618e-01,  2.0762e-02,  6.8405e-02,  5.0225e-01,  2.7702e-01,\n",
      "          3.4498e-01,  3.8492e-01,  3.7485e-01,  5.3566e-01,  2.0073e-01,\n",
      "         -1.1158e-01, -7.8162e-02, -8.7606e-02,  3.8554e-01, -3.7389e-02,\n",
      "         -6.6470e-02, -2.6200e-02, -5.0652e-01, -1.0175e-01,  1.3042e-01,\n",
      "          1.9263e-01,  2.2472e-01, -1.6208e-01, -3.5516e-01, -4.3280e-01,\n",
      "         -6.2472e-01,  2.2809e-01,  1.3806e-01,  3.1072e-02,  1.3830e-01,\n",
      "         -4.8971e-01, -3.4655e-01, -5.9134e-01, -5.6867e-01,  5.5598e-01,\n",
      "          2.4702e-01,  1.4476e-01,  4.4299e-01, -2.0375e-01,  8.3560e-02,\n",
      "         -6.9454e-01, -2.2301e-01],\n",
      "        [ 1.8392e-02,  9.4461e-02,  2.2469e-01, -1.2817e-01,  3.2559e-01,\n",
      "         -1.4317e-01,  1.2328e-01, -2.4463e-01, -2.1019e-02, -4.2470e-01,\n",
      "          5.6995e-01,  1.1416e-01,  4.8530e-02, -3.2086e-01, -2.2003e-01,\n",
      "          2.9752e-01, -5.7682e-01,  7.4473e-01,  5.0523e-01,  1.1241e-01,\n",
      "          1.6501e-01, -3.7768e-01,  4.9620e-02, -1.5348e-01, -2.7349e-01,\n",
      "          6.1214e-02,  3.6387e-01, -1.9729e-01, -1.1053e-02, -3.7251e-01,\n",
      "         -9.1471e-02, -5.7938e-02, -1.5427e-01,  1.8660e-01, -5.5417e-02,\n",
      "         -2.1709e-01,  1.0893e-01, -1.5393e-01,  1.5450e-01, -7.9923e-02,\n",
      "          9.7333e-02,  1.4984e-01],\n",
      "        [ 1.3325e-01, -5.0980e-03, -1.1436e-01,  1.6021e-01,  5.6631e-02,\n",
      "         -2.0954e-01, -1.5098e-01, -6.2034e-01, -5.5523e-01, -8.6943e-01,\n",
      "         -6.0428e-02,  4.2709e-01,  9.6552e-02,  1.6560e-01,  7.5325e-02,\n",
      "         -2.5307e-01, -6.5244e-02, -6.8104e-02, -6.5391e-02,  5.0240e-01,\n",
      "          7.1772e-02, -2.6080e-01, -4.5204e-04,  3.2989e-02,  5.3156e-02,\n",
      "          5.8964e-01, -3.1498e-02,  1.6681e-01,  1.0930e-01, -3.8218e-01,\n",
      "          5.0046e-02,  3.6229e-01,  1.1000e-01,  1.1112e+00, -2.1177e-02,\n",
      "         -2.3997e-01, -1.4779e-02, -3.9478e-01,  1.3659e-01,  3.6376e-01,\n",
      "         -2.6581e-02,  9.3881e-01],\n",
      "        [ 4.4413e-02, -2.7135e-02, -6.5422e-01, -6.9428e-02, -4.2300e-01,\n",
      "         -2.4707e-01, -5.8285e-01, -3.3186e-01, -7.4725e-01, -3.0292e-01,\n",
      "         -4.4641e-01,  2.3745e-01, -8.2598e-02,  8.4153e-02,  4.3382e-01,\n",
      "          8.0176e-02,  8.2954e-01,  3.2223e-01, -5.3160e-01,  2.8289e-01,\n",
      "         -1.0379e-01, -9.4275e-02, -1.6017e-01, -3.1811e-01, -1.8777e-01,\n",
      "         -1.2273e-01, -3.1018e-01,  2.4007e-01,  7.5505e-02, -1.7640e-01,\n",
      "          1.0766e-01, -9.6442e-02, -1.2114e-01,  1.4603e-01, -3.0958e-01,\n",
      "          1.8604e-01,  4.5392e-02, -2.4295e-01,  1.3359e-01, -5.9998e-02,\n",
      "          1.2062e-02,  2.4720e-01],\n",
      "        [-2.5432e-02,  1.2036e-01,  2.3046e-01,  2.9956e-01,  3.8882e-02,\n",
      "          2.3035e-01, -4.7994e-01,  1.1934e-01, -5.5557e-01,  4.3533e-01,\n",
      "          3.2790e-01,  1.5954e-01,  7.5093e-02,  3.6989e-01, -3.3558e-02,\n",
      "         -1.2345e-01, -1.7726e-01, -2.8807e-01, -5.7129e-02,  5.3355e-02,\n",
      "          6.3863e-02,  2.5992e-01,  3.4010e-01, -2.7518e-01,  4.1390e-01,\n",
      "         -6.5412e-01, -2.0557e-01,  7.3726e-02, -4.8227e-02,  2.5968e-01,\n",
      "          3.1838e-01, -1.9473e-01,  6.8728e-01, -7.4347e-01, -2.3144e-01,\n",
      "          4.5219e-01, -3.4445e-01,  3.2199e-01, -9.0565e-02,  1.6422e-01,\n",
      "          1.0719e-01, -3.6341e-01],\n",
      "        [ 6.7404e-02,  7.1919e-02,  3.1601e-03,  1.4378e-01, -3.1452e-01,\n",
      "          3.8967e-02, -3.9061e-01,  4.9630e-02, -6.8012e-01,  1.5538e-01,\n",
      "          4.6629e-02,  1.6826e-01,  1.2961e-01,  1.0645e-01, -2.3943e-02,\n",
      "         -1.2515e-02,  2.3783e-02, -4.3219e-01,  1.4558e-01,  1.9786e-01,\n",
      "          1.7128e-03,  2.8390e-01,  2.5774e-01,  3.0392e-02,  5.9779e-01,\n",
      "         -2.2097e-01, -1.0594e-01,  1.5745e-01, -1.9876e-01,  1.7562e-01,\n",
      "          2.2506e-01, -1.4358e-01,  7.9306e-01, -4.3808e-01, -2.1219e-01,\n",
      "          4.6301e-01, -4.7102e-01,  2.6606e-01,  9.6830e-02,  2.6558e-02,\n",
      "          5.2456e-01, -4.1142e-01],\n",
      "        [-3.4036e-02,  8.8049e-03,  3.8232e-01, -3.2298e-01,  5.4380e-01,\n",
      "         -2.9213e-01,  4.5709e-01, -3.0043e-02,  2.4681e-01, -1.1802e-01,\n",
      "          5.5670e-01,  5.3232e-02,  2.0032e-01,  8.2409e-02, -6.3929e-01,\n",
      "          1.4740e-01, -7.5375e-01,  1.8193e-01,  4.9033e-01,  1.1427e-01,\n",
      "          6.0232e-02,  1.0432e-01,  2.1498e-02,  8.7218e-02, -1.0673e-01,\n",
      "         -1.1474e-01,  4.2984e-01, -4.5186e-02,  9.2193e-03, -8.1275e-02,\n",
      "         -2.2626e-01, -3.4587e-02, -1.4090e-01, -4.1074e-02,  3.7956e-01,\n",
      "         -3.4962e-02,  1.6367e-01, -1.3370e-01,  1.1800e-01, -8.6242e-02,\n",
      "          9.4485e-02, -5.7603e-02],\n",
      "        [-8.3891e-02, -3.0149e-02, -1.7744e-01, -1.6198e-01, -6.2193e-02,\n",
      "         -3.4268e-01, -4.8613e-02, -6.8167e-01, -1.4557e-01, -5.3373e-01,\n",
      "          2.1734e-01, -1.4232e-01,  1.1463e-01, -1.7530e-01, -1.3771e-01,\n",
      "          2.8395e-01, -2.1605e-02,  8.3814e-01,  2.3257e-01, -4.5178e-02,\n",
      "         -1.2175e-01, -3.1242e-01,  6.8976e-02, -2.1299e-01, -4.6847e-02,\n",
      "          2.1919e-01,  1.1549e-01, -8.5192e-02,  5.1260e-02, -2.3238e-01,\n",
      "          1.1982e-01,  5.7680e-02, -1.6163e-02,  3.5353e-01, -1.0512e-01,\n",
      "         -1.1958e-01, -8.5918e-02, -5.0586e-01, -1.3159e-01, -8.1344e-02,\n",
      "         -2.5060e-02,  1.0883e-02],\n",
      "        [ 9.9773e-02,  7.1990e-02, -3.3077e-01, -1.7818e-01, -3.3253e-01,\n",
      "         -4.3329e-01, -2.5917e-01, -6.9167e-01, -4.3021e-01, -5.7395e-01,\n",
      "         -5.0964e-02, -8.8225e-02,  7.2680e-02, -3.0956e-01,  1.4514e-01,\n",
      "          3.6355e-01,  8.5956e-02,  6.4411e-01, -1.2646e-02,  1.3290e-01,\n",
      "          4.5269e-02, -1.2453e-01, -2.1113e-01, -9.0595e-02, -1.3614e-01,\n",
      "          6.4903e-02,  1.1820e-01,  9.0383e-02,  2.2653e-02, -2.8433e-01,\n",
      "         -2.8216e-02, -9.2682e-02,  5.4556e-02,  1.6103e-01,  5.0361e-02,\n",
      "          3.7559e-02,  1.5132e-01, -1.5535e-01, -5.3048e-02, -8.5542e-02,\n",
      "          1.0924e-02,  2.1536e-01],\n",
      "        [ 1.5169e-01,  1.2918e-01,  6.5755e-02,  6.4921e-02,  1.1946e-01,\n",
      "          5.4144e-02,  8.0876e-02,  3.0328e-01, -3.7410e-02,  1.8878e-01,\n",
      "          7.1841e-02,  2.2159e-01,  6.2756e-02,  3.8810e-01,  7.7207e-04,\n",
      "         -1.9415e-01, -5.6298e-02, -7.4505e-01,  1.2760e-02,  3.7075e-02,\n",
      "          6.2398e-02,  3.3837e-01, -9.5976e-02, -2.0531e-01, -8.3158e-02,\n",
      "         -7.3909e-01,  1.2269e-01,  2.4934e-01,  7.2937e-02,  4.0759e-01,\n",
      "         -1.1969e-01, -3.5512e-01, -1.8957e-01, -7.5581e-01,  4.1491e-01,\n",
      "          4.2430e-01,  5.6945e-02,  4.8637e-01, -1.5711e-01,  5.7606e-02,\n",
      "         -2.6420e-01, -5.6752e-01],\n",
      "        [ 1.5364e-01,  3.4454e-02,  2.1400e-01,  5.3101e-01,  2.8580e-01,\n",
      "          1.5041e-01,  1.0378e-01,  3.8242e-01,  1.0550e-01,  2.9635e-01,\n",
      "          1.2522e-02, -4.8991e-02, -9.1929e-02,  2.0887e-01, -5.6555e-02,\n",
      "         -1.0143e-01, -1.8462e-01, -5.4998e-01, -1.3916e-01,  1.6753e-01,\n",
      "          2.7458e-01,  4.0700e-01, -5.8537e-03, -2.6505e-01, -2.5986e-01,\n",
      "         -7.4422e-01,  6.6498e-02,  4.9424e-02,  4.2269e-02,  4.0687e-01,\n",
      "         -1.9097e-01, -4.1545e-01, -2.8622e-01, -9.3294e-01,  3.9234e-01,\n",
      "          4.3018e-01,  1.2009e-01,  5.9657e-01, -1.3563e-02,  1.3912e-01,\n",
      "         -4.1502e-01, -4.2168e-01],\n",
      "        [-5.2810e-02, -5.2794e-02, -1.6421e-01,  4.9203e-01, -7.5072e-02,\n",
      "          2.4073e-01,  2.1328e-03, -8.0540e-02, -2.1843e-01, -5.4112e-02,\n",
      "         -3.5776e-01,  2.0831e-01,  1.3685e-01, -1.7923e-02,  3.3119e-01,\n",
      "          1.5500e-01,  5.2196e-01,  4.0208e-01, -5.4866e-01, -9.0563e-02,\n",
      "         -9.7022e-02, -1.4672e-01, -1.2593e-01, -4.7712e-01, -6.9014e-01,\n",
      "         -7.5847e-01, -3.8313e-01, -1.7114e-02,  2.2565e-01,  5.0888e-02,\n",
      "         -2.1553e-01, -2.1588e-01, -7.6273e-01, -5.0333e-01,  5.3212e-02,\n",
      "          3.4826e-02,  1.8955e-01, -1.3687e-02,  2.1620e-01,  2.8838e-02,\n",
      "         -2.1829e-01, -9.5750e-02],\n",
      "        [-1.2825e-01, -8.0742e-02, -6.9902e-02,  3.1010e-01,  1.2359e-01,\n",
      "         -1.3061e-01, -3.4027e-01, -3.8503e-01, -5.6524e-01, -7.5913e-01,\n",
      "          3.6032e-02,  5.6379e-01,  1.2590e-01,  6.5156e-02,  2.1486e-02,\n",
      "         -1.7623e-01, -3.0652e-02, -2.2950e-01, -7.4662e-02,  5.6774e-01,\n",
      "         -3.0796e-02, -3.1714e-01,  2.4853e-01, -1.1914e-01,  2.4691e-01,\n",
      "          6.8654e-01, -1.2341e-01,  2.0396e-01, -1.2297e-01, -2.5879e-01,\n",
      "          3.4845e-01,  3.4689e-01,  2.1479e-01,  1.0451e+00, -2.6876e-01,\n",
      "         -1.8954e-01, -1.8241e-01, -3.0117e-01,  2.5783e-01,  3.3778e-01,\n",
      "          9.6721e-02,  9.0447e-01],\n",
      "        [-5.0909e-02, -4.6606e-02, -1.3202e-01, -9.0240e-02, -3.5496e-01,\n",
      "         -2.0903e-02, -1.1058e-01,  4.5522e-01,  2.1918e-02,  2.1944e-01,\n",
      "         -6.2131e-02, -7.9288e-02,  1.2745e-01, -1.1119e-01,  4.1602e-01,\n",
      "         -2.4308e-01,  4.9823e-01, -5.4481e-01,  7.4094e-02, -9.0029e-02,\n",
      "         -3.0724e-01, -7.5968e-02, -2.5255e-01,  2.0181e-01, -2.2738e-02,\n",
      "          2.7970e-01,  9.0196e-02, -1.6288e-01, -2.5653e-01,  2.6858e-02,\n",
      "         -9.4513e-02,  2.4320e-01,  8.6595e-02, -1.7370e-02,  5.4109e-02,\n",
      "          1.1750e-02,  3.3341e-02,  1.0613e-01, -2.2689e-02, -7.4559e-02,\n",
      "          2.3004e-01, -6.6087e-02],\n",
      "        [-8.4477e-02,  4.9636e-02, -3.9631e-01,  1.3746e-01, -5.3012e-02,\n",
      "         -2.4524e-01, -2.3677e-01, -5.7923e-01, -3.9192e-01, -5.4370e-01,\n",
      "         -1.2962e-01,  1.8938e-01,  9.4703e-03, -2.3870e-01,  3.2548e-01,\n",
      "          2.9013e-01,  5.2594e-01,  7.9381e-01, -1.5571e-01,  4.4346e-02,\n",
      "         -2.6340e-01, -3.9186e-01,  4.3616e-02, -2.5273e-01, -1.6335e-01,\n",
      "         -1.5364e-02, -1.4716e-01, -2.6345e-02, -1.5862e-01, -2.6232e-01,\n",
      "          2.5417e-01,  6.0122e-02,  8.4138e-02,  3.4068e-01, -4.0519e-01,\n",
      "         -3.4887e-01, -2.2817e-01, -5.2324e-01,  1.0702e-02, -1.0326e-01,\n",
      "         -3.3568e-02,  2.3391e-01],\n",
      "        [ 3.9215e-02,  8.9153e-02, -1.5220e-01,  1.4809e-01, -2.9625e-01,\n",
      "         -7.6674e-02, -4.2212e-01, -4.1978e-01, -3.3095e-01, -2.7853e-01,\n",
      "         -2.5703e-01,  9.8939e-02,  1.3039e-01, -7.3253e-03,  3.8948e-01,\n",
      "          1.9502e-01,  7.7116e-01,  4.1105e-01, -6.4254e-01,  1.0523e-01,\n",
      "         -1.3209e-01, -9.7819e-02,  2.0365e-01, -1.2456e-01, -5.2731e-02,\n",
      "          4.9335e-02, -6.3803e-01, -1.8487e-01, -2.5011e-02, -1.7944e-01,\n",
      "          1.2020e-01,  1.3688e-03,  1.1932e-02,  2.9133e-01, -7.1386e-01,\n",
      "         -2.3496e-01, -9.9806e-02, -5.5636e-01, -6.1794e-02, -2.6640e-01,\n",
      "          1.7927e-01, -1.9679e-03]]), '0.bias': tensor([-0.4220, -0.5445, -0.4404, -0.6878, -0.4839, -0.4171, -0.2870, -0.4487,\n",
      "        -0.2908, -0.2196, -0.3928, -0.5759, -0.6199, -0.2344, -0.3236, -0.5371,\n",
      "        -0.2307, -0.1627, -0.5623, -0.4659]), '3.weight': tensor([[ 0.5182, -0.1479, -0.8876, -0.7149,  0.6329, -0.8617,  0.3559, -0.5998,\n",
      "          0.2587,  0.9036, -0.8467, -0.4615, -0.6520,  0.9170,  1.0762, -0.2144,\n",
      "          0.6005,  0.6468, -0.7864, -0.6660],\n",
      "        [ 0.7667,  0.7013,  1.3092,  0.7402,  0.6692,  0.8013,  0.8614,  0.8773,\n",
      "          0.9222,  0.3507,  0.7074,  0.5095,  0.6025,  0.2027,  0.6786,  0.6614,\n",
      "          0.5745, -0.5443,  0.9689,  0.5755],\n",
      "        [ 1.1177,  0.6701, -0.5208, -0.1295,  1.1219, -1.0858,  0.8445, -0.0510,\n",
      "          0.9034,  0.7743, -0.7791, -0.4309, -0.0523,  0.5496,  1.1043,  0.5277,\n",
      "          1.0322, -0.2018, -0.4770, -0.8610],\n",
      "        [-0.2576,  0.2471,  1.1716,  0.8449, -0.3229,  0.9204, -0.0700,  0.7165,\n",
      "         -0.0663, -0.3997,  0.9673,  0.6735,  0.7166, -0.6605, -0.5473,  0.2408,\n",
      "         -0.2744, -0.7090,  0.8569,  0.8015],\n",
      "        [ 0.9526,  0.9740,  1.5941,  0.9472,  0.8726,  0.9610,  0.8291,  0.9769,\n",
      "          0.9255,  0.7432,  0.9466,  0.6163,  0.6369,  0.0527,  0.8332,  0.9034,\n",
      "          0.9278, -0.7244,  1.2855,  0.7201],\n",
      "        [ 1.0474,  0.4936, -0.3231, -0.1811,  0.8738, -1.0498,  0.7052, -0.1543,\n",
      "          0.6935,  0.7365, -0.7145, -0.4783, -0.2227,  0.4715,  0.9114,  0.3853,\n",
      "          0.8383, -0.1804, -0.4942, -0.8243],\n",
      "        [-0.7629,  0.0857,  1.8176,  0.8089, -0.6991,  1.6596, -0.4090,  0.8869,\n",
      "         -0.3874, -0.5217,  1.5805,  1.1034,  0.8575, -0.7561, -0.9296, -0.0031,\n",
      "         -0.7187, -0.3262,  1.5093,  1.4313],\n",
      "        [ 0.4220, -0.1878, -0.9730, -0.6815,  0.4278, -0.8012,  0.1970, -0.6999,\n",
      "          0.2686,  0.6826, -0.6755, -0.4986, -0.6398,  0.8322,  0.8098, -0.2227,\n",
      "          0.4403,  0.7484, -0.8414, -0.6613],\n",
      "        [-0.8882,  0.2080,  2.1495,  1.1427, -0.8510,  2.3861, -0.5149,  1.3799,\n",
      "         -0.7235, -0.7350,  1.7503,  1.5824,  1.0718, -0.9388, -1.0769, -0.0125,\n",
      "         -0.7233, -0.4464,  1.9079,  1.8408],\n",
      "        [ 1.3857,  0.6533, -0.3901,  0.1226,  1.1406, -1.0534,  0.9022,  0.0456,\n",
      "          1.0999,  0.8339, -0.8317, -0.5326, -0.0537,  0.5382,  1.1872,  0.5774,\n",
      "          0.9413, -0.1871, -0.3693, -0.8535]]), '3.bias': tensor([ 0.6307, -0.2314, -0.1769, -0.1524, -0.2347, -0.1535,  0.1979,  0.6289,\n",
      "         0.2229, -0.1553]), '6.weight': tensor([[ 0.7811,  0.6349,  1.6373, -0.9170,  0.8249,  1.7278, -1.4043,  0.5742,\n",
      "         -0.8167,  1.3874],\n",
      "        [-2.0580,  1.0606, -0.3416,  2.1285,  1.1532, -0.7105,  1.2910, -2.4485,\n",
      "          1.1193, -0.4136],\n",
      "        [ 0.6665, -2.7295, -1.6106, -1.4567, -1.7721, -1.9098, -0.1439,  0.6890,\n",
      "         -0.0674, -1.7071],\n",
      "        [-2.0384, -1.2961, -0.7478, -1.5431, -0.7900, -0.7099, -1.2368, -2.1327,\n",
      "         -1.4863, -0.8339],\n",
      "        [-2.0794, -1.5892, -0.6640, -1.1857, -1.4175, -0.8709, -1.9858, -2.2195,\n",
      "         -2.0703, -0.8935]]), '6.bias': tensor([-2.3444, -1.1504,  3.2017, -3.4598, -2.8285])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14872\\285514669.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load(r'F:\\TTCS\\model\\keypoint_classifier\\best_model.pth')\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load(r'F:\\TTCS\\model\\keypoint_classifier\\best_model.pth')\n",
    "print(best_model)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7071086,
     "sourceId": 11306655,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
